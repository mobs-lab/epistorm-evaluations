{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53044182-bf0b-4dcd-b811-eee167b40de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from epiweeks import Week\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import os\n",
    "from functools import reduce\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc9f67dd-2b72-4f6f-bad4-584da6ada67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.9\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98848f4d-50db-457e-94cf-aaf8d913e35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.4'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b21d823-76a1-420b-a2ef-dd7b8318c14f",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "At least for now, I'm using Clara's functions for pulling predictions and surveillance data. We will probably want to set up the Flusight repo as a submodule and pull new data on a schedule. \n",
    "\n",
    "\n",
    "Here's how it's currently implemented:\n",
    "1. For each selected model and date, we save each forecast submission as-is in parquet format.\n",
    "2. For each selected model and date, we read the corresponding parquets and concatenate into a single data frame.\n",
    "3. We instantiate the Forecast_Eval class with a given start and end week, and call the format_forecasts_all method on the concatenated predictions dataframe, which filters for quantile predictions, removes dates after the end week, enforces datatypes, and returns the formatted dataframe to be used in scoring. (NOTE: start week is not used, what's up with that?)\n",
    "\n",
    "Automating this workflow will probably involve:\n",
    "1. Calculate scorings for all historical data once and save.\n",
    "2. Set up the Flusight repo as a submodule and pull new data on a schedule.\n",
    "3. It might be possible to keep the implementation with parquets if github can run it (pulling from flusight repo and writing and reading parquets within a python script). This would eliminate (2.) and could avoid problems when making epistorm-evaluations a submodule of epistorm-dashboard, since it also has the flusight repo as a submodule.\n",
    "4. Every time new data is pulled, calculate scoring for only the new data and append to existing scoring files. Do we need to re-calculate scores for previous weeks in case surveillance data changes retrospectively?\n",
    "5. We might generate separate files with transformed data for charts in the dashboard, if so either:\n",
    "    1. Do that here, add epistorm-evaluations as a submodule of epistorm-dashboard, and pull the transformed scores on a schedule.\n",
    "    2. Add epistorm-evaluations as a submodule of epistorm-dashboard, pull the scoring files on a schedule, and trigger a script within epistorm-dashboard to create the transformed scoring files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b823c2e-43ad-4f25-9ec2-68864e3cffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to download flusight model predictions and surveillance data\n",
    "\n",
    "def pull_flusight_predictions(model,date):\n",
    "    \"\"\"pull_flusight_predictions. Load predictions of the model saved by the Flusight Forecast hub\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : str\n",
    "        Model name on the\n",
    "    dates : list or string\n",
    "        List of potential dates in the iso format, e.g., 'yyyy-mm-dd', for the submission.\n",
    "    \"\"\"\n",
    "    predictions = None\n",
    "    \n",
    "    url = f\"https://raw.githubusercontent.com/cdcepi/Flusight-forecast-hub/main/model-output/{model}/{date}-{model}\"\n",
    "    for ext in [\".csv\",\".gz\",\".zip\",\".csv.zip\",\".csv.gz\"]:\n",
    "        try:\n",
    "            predictions = pd.read_csv(url+ext,dtype={'location':str},parse_dates=['target_end_date'])\n",
    "        except:\n",
    "            pass\n",
    "    if predictions is None:\n",
    "        print(f\"Data for model {model} and date {date} unavailable\")\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def pull_surveillance_data():\n",
    "    \"\"\"pull_surveillance_data. Load hospitalization admissions surveillance data\n",
    "    \"\"\"\n",
    "    \n",
    "    url = f\"https://raw.githubusercontent.com/cdcepi/FluSight-forecast-hub/main/target-data/target-hospital-admissions.csv\"\n",
    "    return pd.read_csv(url, dtype={'location':str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cd36fa6-150b-47d2-8ce7-3e82281b708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and save surveillance data\n",
    "surv = pull_surveillance_data() \n",
    "surv = surv[(surv.date >= '2023-06-01') & (surv.date<='2024-05-01')] # filtered dates after june 2023 since we are only looking at 2023-24 season\n",
    "surv.to_parquet(f\"./dat/target-hospital-admissions.pq\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "475e7a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = pd.read_parquet(f\"./dat/target-hospital-admissions.pq\")\n",
    "surv['Unnamed: 0'] = 0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "745b3b96-e522-432b-aac2-b5ecdabd539f",
   "metadata": {},
   "source": [
    "### From Clara's notebook\n",
    "# download and save forecasts for specified submission week (date) and for specified models from flusight github\n",
    "\n",
    "dates = [ '2024-04-06', '2024-04-13', '2024-04-20', '2024-04-27']\n",
    "\n",
    "models = ['CADPH-FluCAT_Ensemble', 'CEPH-Rtrend_fluH',  'CMU-TimeSeries', 'CU-ensemble', 'FluSight-baseline',\n",
    "          'FluSight-ensemble','FluSight-equal_cat', 'FluSight-lop_norm', 'GH-model', 'GT-FluFNP', 'ISU_NiemiLab-ENS', \n",
    "          'ISU_NiemiLab-NLH','ISU_NiemiLab-SIR', 'LUcompUncertLab-chimera', 'LosAlamos_NAU-CModel_Flu', \n",
    "          'MIGHTE-Nsemble','MOBS-GLEAM_FLUH', 'NIH-Flu_ARIMA', 'PSI-PROF', 'SGroup-RandomForest', 'SigSci-CREG', \n",
    "          'SigSci-TSENS','Stevens-GBR', 'UGA_flucast-Copycat', 'UGA_flucast-INFLAenza', 'UGA_flucast-OKeeffe', \n",
    "          'UGuelph-CompositeCurve', 'UGuelphensemble-GRYPHON', 'UM-DeepOutbreak', 'UMass-flusion', \n",
    "          'UMass-trends_ensemble', 'UNC_IDD-InfluPaint', 'UVAFluX-Ensemble', 'VTSanghani-Ensemble', 'cfa-flumech',\n",
    "          'cfarenewal-cfaepimlight', 'fjordhest-ensemble', 'NU_UCSD-GLEAM_AI_FLUH', 'PSI-PROF_beta',\n",
    "          'JHU_CSSE-CSSE_Ensemble', 'FluSight-national_cat', 'FluSight-ens_q_cat', 'FluSight-baseline_cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce7163e-f762-47c3-b3a4-e6a7f322f9bf",
   "metadata": {},
   "source": [
    "Clara was hard-coding dates to pull each time she manually ran this notebook. Since we want scores for all historical data, I'm using all dates from the surveillance file. I don't think this should cause a bug or miss any data but it would be good to have a second opinion.\n",
    "\n",
    "Once we've got all the historical scores, if we're only scoring new predictions every week, should we only use the most recent surveillance date?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2055db05-7fe1-4a76-ae2f-1a1e0a2c8cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting all target dates that exist in the surveillance file\n",
    "dates = pd.unique(surv.date)\n",
    "\n",
    "#selecting just models used in the dashboard for now\n",
    "#will need to expand eventually whether we keep the parquet implementation or pull files from the flusight repo as a submodule\n",
    "models = ['CEPH-Rtrend_fluH', 'FluSight-baseline', 'FluSight-ensemble', 'MIGHTE-Nsemble', 'MOBS-GLEAM_FLUH', 'NU_UCSD-GLEAM_AI_FLUH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6805cfdd-aac5-42e7-9d38-74097fafc0c8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for model CEPH-Rtrend_fluH and date 2023-10-07 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model CEPH-Rtrend_fluH and date 2023-09-30 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model CEPH-Rtrend_fluH and date 2023-09-23 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model CEPH-Rtrend_fluH and date 2023-09-16 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model CEPH-Rtrend_fluH and date 2023-09-09 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model CEPH-Rtrend_fluH and date 2023-09-02 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model CEPH-Rtrend_fluH and date 2023-08-26 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model CEPH-Rtrend_fluH and date 2023-08-19 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model CEPH-Rtrend_fluH and date 2023-08-12 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model CEPH-Rtrend_fluH and date 2023-08-05 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model CEPH-Rtrend_fluH and date 2023-07-29 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model CEPH-Rtrend_fluH and date 2023-07-22 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model CEPH-Rtrend_fluH and date 2023-07-15 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model CEPH-Rtrend_fluH and date 2023-07-08 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model CEPH-Rtrend_fluH and date 2023-07-01 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model CEPH-Rtrend_fluH and date 2023-06-24 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model CEPH-Rtrend_fluH and date 2023-06-17 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model CEPH-Rtrend_fluH and date 2023-06-10 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model CEPH-Rtrend_fluH and date 2023-06-03 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-baseline and date 2023-10-07 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-baseline and date 2023-09-30 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-baseline and date 2023-09-23 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-baseline and date 2023-09-16 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-baseline and date 2023-09-09 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-baseline and date 2023-09-02 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-baseline and date 2023-08-26 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-baseline and date 2023-08-19 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-baseline and date 2023-08-12 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-baseline and date 2023-08-05 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-baseline and date 2023-07-29 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-baseline and date 2023-07-22 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-baseline and date 2023-07-15 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-baseline and date 2023-07-08 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-baseline and date 2023-07-01 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-baseline and date 2023-06-24 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-baseline and date 2023-06-17 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-baseline and date 2023-06-10 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-baseline and date 2023-06-03 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-ensemble and date 2023-10-07 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-ensemble and date 2023-09-30 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-ensemble and date 2023-09-23 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-ensemble and date 2023-09-16 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-ensemble and date 2023-09-09 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-ensemble and date 2023-09-02 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-ensemble and date 2023-08-26 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-ensemble and date 2023-08-19 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-ensemble and date 2023-08-12 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-ensemble and date 2023-08-05 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-ensemble and date 2023-07-29 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-ensemble and date 2023-07-22 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-ensemble and date 2023-07-15 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-ensemble and date 2023-07-08 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-ensemble and date 2023-07-01 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-ensemble and date 2023-06-24 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-ensemble and date 2023-06-17 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-ensemble and date 2023-06-10 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model FluSight-ensemble and date 2023-06-03 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MIGHTE-Nsemble and date 2023-10-07 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MIGHTE-Nsemble and date 2023-09-30 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MIGHTE-Nsemble and date 2023-09-23 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MIGHTE-Nsemble and date 2023-09-16 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MIGHTE-Nsemble and date 2023-09-09 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MIGHTE-Nsemble and date 2023-09-02 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MIGHTE-Nsemble and date 2023-08-26 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MIGHTE-Nsemble and date 2023-08-19 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MIGHTE-Nsemble and date 2023-08-12 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MIGHTE-Nsemble and date 2023-08-05 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MIGHTE-Nsemble and date 2023-07-29 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MIGHTE-Nsemble and date 2023-07-22 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MIGHTE-Nsemble and date 2023-07-15 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MIGHTE-Nsemble and date 2023-07-08 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MIGHTE-Nsemble and date 2023-07-01 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MIGHTE-Nsemble and date 2023-06-24 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MIGHTE-Nsemble and date 2023-06-17 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for model MIGHTE-Nsemble and date 2023-06-10 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MIGHTE-Nsemble and date 2023-06-03 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MOBS-GLEAM_FLUH and date 2023-10-07 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MOBS-GLEAM_FLUH and date 2023-09-30 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MOBS-GLEAM_FLUH and date 2023-09-23 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MOBS-GLEAM_FLUH and date 2023-09-16 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MOBS-GLEAM_FLUH and date 2023-09-09 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MOBS-GLEAM_FLUH and date 2023-09-02 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MOBS-GLEAM_FLUH and date 2023-08-26 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MOBS-GLEAM_FLUH and date 2023-08-19 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MOBS-GLEAM_FLUH and date 2023-08-12 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MOBS-GLEAM_FLUH and date 2023-08-05 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MOBS-GLEAM_FLUH and date 2023-07-29 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MOBS-GLEAM_FLUH and date 2023-07-22 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MOBS-GLEAM_FLUH and date 2023-07-15 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MOBS-GLEAM_FLUH and date 2023-07-08 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MOBS-GLEAM_FLUH and date 2023-07-01 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MOBS-GLEAM_FLUH and date 2023-06-24 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MOBS-GLEAM_FLUH and date 2023-06-17 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MOBS-GLEAM_FLUH and date 2023-06-10 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model MOBS-GLEAM_FLUH and date 2023-06-03 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-11-25 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-11-18 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-11-11 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-11-04 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-10-28 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-10-21 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-10-14 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-10-07 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-09-30 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-09-23 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-09-16 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-09-09 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-09-02 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-08-26 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-08-19 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-08-12 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-08-05 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-07-29 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-07-22 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-07-15 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-07-08 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-07-01 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-06-24 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-06-17 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-06-10 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n",
      "Data for model NU_UCSD-GLEAM_AI_FLUH and date 2023-06-03 unavailable\n",
      "'NoneType' object has no attribute 'to_parquet'\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    for date in dates:\n",
    "        try:\n",
    "            predictions = pull_flusight_predictions(model,date)\n",
    "\n",
    "            predictions.to_parquet(f'./dat/{model}_{date}.pq', index=False)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ebbec-69bc-4dbf-973f-048728983389",
   "metadata": {},
   "source": [
    "# Classes and Methods\n",
    "\n",
    "It would be nice to have proper docstrings for everything here. Should rename Flusight_2324 to something else. Changed to Forecast_Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b66a9f77-bc14-430c-bd38-cfaa2218054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for calculating scores of the forecasts against the surveillance data\n",
    "\n",
    "\n",
    "class Forecast_Eval:\n",
    "    \"\"\" Used for scoring and evaluating flu forecasting predictions from the Flusight challenge for the \n",
    "        2023-24 season\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, obsdf, target,  start_week = False, end_week = False):\n",
    "        self.df = df # pandas Dataframe: input dataframe of forecasts with all scenarios, locations, and quantiles\n",
    "        self.obsdf = obsdf # pandas Dataframe: input of surveillance data of interest\n",
    "        self.target = target # str: target metric of interest (case, death, hospitalization)\n",
    "        self.start_week = start_week # epiweek: beginning of observations of interest\n",
    "        self.end_week = end_week # epiweek: end of observations of interest\n",
    "        \n",
    "    def process_observations(self, data, value_col='value', t_col='date', other_ind_cols=None):\n",
    "        \"\"\"\n",
    "    \n",
    "        Parameters:\n",
    "        - data: pd.DataFrame\n",
    "            The input data containing observational information.\n",
    "        - value_col: str\n",
    "            The column name representing the value column.\n",
    "        - t_col: str\n",
    "            The column name representing the time column.\n",
    "        - other_ind_cols: list\n",
    "            A list of additional independent columns for sorting.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame with additional helper functions for accessing specific columns.\n",
    "        \"\"\"\n",
    "        # Ensure required columns are present\n",
    "        if value_col not in data.columns or t_col not in data.columns:\n",
    "            raise ValueError(f\"DataFrame must contain '{value_col}' and '{t_col}' columns.\")\n",
    "\n",
    "        # Prepare independent columns and sort data\n",
    "        ind_cols = [t_col] + (other_ind_cols if other_ind_cols else [])\n",
    "        sorted_data = data.sort_values(by=ind_cols).reset_index(drop=True)\n",
    "\n",
    "        # Define helper functions to access specific columns and set them as attributes\n",
    "        sorted_data.get_value = lambda: sorted_data[value_col].to_numpy()\n",
    "        sorted_data.get_t = lambda: sorted_data[t_col].to_numpy()\n",
    "        sorted_data.get_x = lambda: sorted_data[ind_cols].to_numpy()\n",
    "        sorted_data.get_unique_x = lambda: np.unique(np.array(sorted_data[ind_cols].to_numpy(), dtype=str), axis=0)\n",
    "\n",
    "        return sorted_data      \n",
    "            \n",
    "            \n",
    "    def get_observations(self, target_location):\n",
    "        \"\"\" get_observations. Load and filter surveillance data for a certain location.\n",
    "        Parameters\n",
    "        ----------\n",
    "        target_location : str\n",
    "            location to filter surveillance data by\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.target == 'hosp':\n",
    "            target_obs = 'hospitalization'\n",
    "        else:\n",
    "            target_obs = self.target\n",
    "            \n",
    "        # read in observations dataframe\n",
    "        \n",
    "        observations = self.obsdf.copy().drop(columns= ['Unnamed: 0', 'weekly_rate'])\n",
    "        observations['date'] = pd.to_datetime(observations['date'])\n",
    "\n",
    "        #filter start - end week\n",
    "        if self.start_week:\n",
    "            observations = observations[(observations['date'] >= pd.to_datetime(self.start_week.startdate())) ]\n",
    "            \n",
    "        if self.end_week:\n",
    "            observations = observations[(observations['date'] <= pd.to_datetime(self.end_week.enddate()))]\n",
    "                                \n",
    "        #filter location\n",
    "        observations = observations[observations['location'] == target_location]\n",
    "\n",
    "        #aggregate to weekly\n",
    "        observations = observations.groupby(['location', pd.Grouper(key='date', freq='W-SAT')]).sum().reset_index()\n",
    "\n",
    "        #transform to Observation object\n",
    "        observations = self.process_observations(observations)\n",
    "\n",
    "        return observations\n",
    "    \n",
    "    def process_predictions(self, data, value_col='value', quantile_col='output_type_id', type_col='output_type',\n",
    "                        t_col='target_end_date', other_ind_cols=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters:\n",
    "        - data: pd.DataFrame\n",
    "            The input data containing prediction information.\n",
    "        - value_col: str\n",
    "            Column label for the predictions' value.\n",
    "        - quantile_col: str\n",
    "            Column label for the predictions' quantile.\n",
    "        - type_col: str\n",
    "            Column label for the type of predictions (e.g., quantile or point).\n",
    "        - t_col: str\n",
    "            Column label for the timestamp of predictions.\n",
    "        - other_ind_cols: list\n",
    "            List of other independent variable columns (e.g., location).\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame with additional helper methods to access specific data arrays or filtered predictions.\n",
    "        \"\"\"\n",
    "        # Ensure required columns are present\n",
    "        if not all(col in data.columns for col in [value_col, quantile_col, t_col]):\n",
    "            raise ValueError(f\"DataFrame must contain '{value_col}', '{quantile_col}', and '{t_col}' columns.\")\n",
    "        if other_ind_cols and not all(col in data.columns for col in other_ind_cols):\n",
    "            raise ValueError(\"DataFrame must contain all specified independent columns.\")\n",
    "\n",
    "        # Define independent columns and sort data\n",
    "        ind_cols = [t_col] + (other_ind_cols if other_ind_cols else [])\n",
    "        sorted_data = data.sort_values(by=ind_cols).reset_index(drop=True)\n",
    "\n",
    "        # Set default value for type column if missing\n",
    "        if type_col not in sorted_data.columns:\n",
    "            sorted_data[type_col] = 'quantile'\n",
    "\n",
    "        # Attach helper methods as attributes of the DataFrame\n",
    "        sorted_data.get_t = lambda: sorted_data[t_col].to_numpy()\n",
    "        sorted_data.get_x = lambda: sorted_data[ind_cols].to_numpy()\n",
    "        sorted_data.get_unique_x = lambda: np.unique(np.array(sorted_data[ind_cols].to_numpy(), dtype=str), axis=0)\n",
    "\n",
    "      \n",
    "\n",
    "        return sorted_data\n",
    "\n",
    "    \n",
    "    def format_forecasts_all(self, dfformat):\n",
    "        \"\"\" format_forecasts_all. Get forecasts into standard format to use for scoring.\n",
    "        Parameters\n",
    "        ----------\n",
    "        dfformat : pandas DataFrame\n",
    "            dataframe of forecast output used for formatting\n",
    "        \"\"\"\n",
    "        \n",
    "        pred = dfformat.copy()\n",
    "        pred = pred[pred.output_type == 'quantile'] # only keep quantile predictions\n",
    "        pred['target_end_date'] = pd.to_datetime(pred['target_end_date']) #make sure dates are in datetime format\n",
    "        if self.start_week:\n",
    "            pred = pred[(pred['target_end_date'] >= pd.to_datetime(self.start_week.startdate()))] # filter dates\n",
    "        \n",
    "        if self.end_week:\n",
    "            pred = pred[(pred['target_end_date'] <= pd.to_datetime(self.end_week.enddate()))] # filter dates\n",
    "        \n",
    "        pred['output_type_id'] = pred[\"output_type_id\"].astype(float) # make sure quantile levels are floats\n",
    "        \n",
    "        return pred\n",
    "        \n",
    "    \n",
    "\n",
    "class Scoring(Forecast_Eval):\n",
    "    \"\"\" calculate score values for probabilistic epidemic forecasts \n",
    "    find WIS, MAPE, and coverage over whole projection window as well as timestamped for every week.\n",
    "    score dataframe must have 'Model' column to differentiate and calculate scores for different models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, obsdf, target, start_week = False, \n",
    "                 end_week = False):\n",
    "        super().__init__(df, obsdf, target, start_week, end_week)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def interval_score(self, observation, lower, upper, interval_range, specify_range_out=False):\n",
    "        \"\"\"interval_score.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : array_like\n",
    "            Vector of observations.\n",
    "        lower : array_like\n",
    "            Prediction for the lower quantile.\n",
    "        upper : array_like\n",
    "            Prediction for the upper quantile.\n",
    "        interval_range : int\n",
    "            Percentage covered by the interval. For instance, if lower and upper correspond to 0.05 and 0.95\n",
    "            quantiles, interval_range is 90.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out : dict\n",
    "            Dictionary containing vectors for the interval scores, but also the dispersion, underprediction and\n",
    "            overprediction.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError:\n",
    "            If the observation, the lower and upper vectors are not the same length or if interval_range is not\n",
    "            between 0 and 100\n",
    "        \"\"\"\n",
    "        if len(lower) != len(upper) or len(lower) != len(observation):\n",
    "            raise ValueError(\"vector shape mismatch\")\n",
    "        if interval_range > 100 or interval_range < 0:\n",
    "            raise ValueError(\"interval range should be between 0 and 100\")\n",
    "\n",
    "        #make sure vector operation works\n",
    "        obs,l,u = np.array(observation),np.array(lower),np.array(upper)\n",
    "\n",
    "        alpha = 1-interval_range/100 #prediction probability outside the interval\n",
    "        dispersion = u - l\n",
    "        underprediction = (2/alpha) * (l-obs) * (obs < l)\n",
    "        overprediction = (2/alpha) * (obs-u) * (obs > u)\n",
    "        score = dispersion + underprediction + overprediction\n",
    "        if not specify_range_out:\n",
    "            out = {'interval_score': score,\n",
    "                   'dispersion': dispersion,\n",
    "                   'underprediction': underprediction,\n",
    "                   'overprediction': overprediction}\n",
    "        else:\n",
    "            out = {f'{interval_range}_interval_score': score,\n",
    "                   f'{interval_range}_dispersion': dispersion,\n",
    "                   f'{interval_range}_underprediction': underprediction,\n",
    "                   f'{interval_range}_overprediction': overprediction}\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    def timestamp_wis(self,observations, predsfilt, interval_ranges =[10,20,30,40,50,60,70,80,90,95,98]):\n",
    "        \"\"\"timestamp_wis.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observations : Observations object\n",
    "            Specialized dateframe for the observations across time.\n",
    "        predictions : Predictions object\n",
    "            Specialized dateframe for the predictions (quantile and point) across time.\n",
    "        interval_ranges : list of int\n",
    "            Percentage covered by each interval. For instance, if interval_range is 90, this corresponds\n",
    "            to the interval for the 0.05 and 0.95 quantiles.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df : DataFrame\n",
    "            DataFrame containing the weighted interval score across time.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError:\n",
    "            If the independent columns do not match for observations and predictions.\n",
    "            If the median is not calculated.\n",
    "            If the point estimate is not included.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        quantiles = np.array(predsfilt.sort_values(by='output_type_id').output_type_id)\n",
    "\n",
    "        qs = []\n",
    "        for q in quantiles:\n",
    "            df = predsfilt[predsfilt.output_type_id==q].sort_values(by='target_end_date')\n",
    "            val = np.array(df.value)\n",
    "            qs.append(val)\n",
    "\n",
    "\n",
    "        Q = np.array(qs) # quantiles array\n",
    "        y = np.array(observations.value) # observations array\n",
    "\n",
    "        # calculate WIS\n",
    "        WIS = np.zeros(len(y))\n",
    "\n",
    "        for i in range(len(quantiles) // 2):\n",
    "            interval_range = 100*(quantiles[-i-1]-quantiles[i])\n",
    "            #print(interval_range)\n",
    "            alpha = 1-(quantiles[-i-1]-quantiles[i])\n",
    "            IS = interval_score(y,Q[i],Q[-i-1],interval_range)\n",
    "            WIS += IS['interval_score']*alpha/2\n",
    "        WIS += 0.5*np.abs(Q[11] - y)\n",
    "\n",
    "        WISlist = np.array(WIS) / (len(interval_ranges) + 0.5)\n",
    "\n",
    "        df = pd.DataFrame({'Model':predsfilt.Model.unique(), 'location':predsfilt.location.unique(),\n",
    "                          'target_end_date':predsfilt.target_end_date.unique(), 'wis':WISlist[0]},index=[0])\n",
    "\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def coverage(self,observation,lower,upper):\n",
    "        \"\"\"coverage. Output the fraction of observations within lower and upper.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : array_like\n",
    "            Vector of observations.\n",
    "        lower : array_like\n",
    "            Prediction for the lower quantile.\n",
    "        upper : array_like\n",
    "            Prediction for the upper quantile.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cov : float\n",
    "            Fraction of observations within the lower and upper bound.\n",
    "\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError:\n",
    "            If the observation, the lower and upper vectors are not the same length.\n",
    "        \"\"\"\n",
    "        if len(lower) != len(upper) or len(lower) != len(observation):\n",
    "            raise ValueError(\"vector shape mismatch\")\n",
    "\n",
    "        #make sure vector operation works\n",
    "        obs,l,u = np.array(observation),np.array(lower),np.array(upper)\n",
    "\n",
    "        return np.mean(np.logical_and(obs >= l, obs <= u))\n",
    "\n",
    "\n",
    "    def all_coverages_from_df(self,observations, predictions, interval_ranges=[10,20,30,40,50,60,70,80,90,95,98],\n",
    "                              **kwargs):\n",
    "        \"\"\"all_coverages_from_df.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observations : DataFrame object\n",
    "            Dateframe for the observations across time.\n",
    "        predictions : DataFrame object\n",
    "            Dateframe for the predictions (intervals) across time.\n",
    "        interval_ranges : list of int\n",
    "            Percentage covered by each interval. For instance, if interval_range is 90, this corresponds\n",
    "            to the interval for the 0.05 and 0.95 quantiles.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out : dict\n",
    "            Dictionary containing the coverage for all interval ranges.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError:\n",
    "            If the independent columns do not match for observations and predictions.\n",
    "        \"\"\"\n",
    "        #verify that the independent variable columns (usually dates and location) matches\n",
    "        # if not np.array_equal(observations.get_unique_x(), predictions.get_unique_x()):\n",
    "            # raise ValueError(\"Values for the independent columns do not match\")\n",
    "\n",
    "        out = dict()\n",
    "        for interval_range in interval_ranges:\n",
    "            q_low,q_upp = round(0.5-interval_range/200,3),round(0.5+interval_range/200,3)\n",
    "            cov = coverage(list(observations.value),\n",
    "                           list(predictions[predictions.output_type_id ==q_low].value),\n",
    "                           list(predictions[predictions.output_type_id==q_upp].value))\n",
    "            out[f'{interval_range}_cov'] = cov\n",
    "        return out\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_mape(self):\n",
    "        \"\"\" get_mape. Calculate MAPE (mean absolute percentage error) for each date of a forecast. If \n",
    "            surveillance data point is equal to zero, the score is undefined (Nan).\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        predictions = self.df.copy()\n",
    "        \n",
    "        # get point forecast, here we say it is the median\n",
    "        predictions = predictions[predictions['output_type_id'] == 0.5] # get point forecast, here we say it is the median\n",
    "\n",
    "        mapedf = pd.DataFrame()\n",
    "\n",
    "        # find mape for a given model and location over projection period\n",
    "        for model in predictions.Model.unique():\n",
    "            for target_location in predictions.location.unique():\n",
    "\n",
    "                    if target_location in ['60','66','69', '72', '78']:\n",
    "                        continue\n",
    "\n",
    "                    observations = self.get_observations(target_location)\n",
    "                    \n",
    "\n",
    "                    pred = predictions[(predictions.location == target_location) & (predictions.Model==model)]\n",
    "                    pred = process_predictions(pred, t_col = 'target_end_date',quantile_col='output_type_id')\n",
    "                    \n",
    "                    observations = observations[observations.date.isin(pred.target_end_date.unique())]\n",
    "\n",
    "                    n = observations.shape[0]\n",
    "\n",
    "                    realvals = list(observations.value)\n",
    "                    predvals = list(pred.value)\n",
    "\n",
    "                    \n",
    "                    if len(predvals) == 0:\n",
    "                        continue\n",
    "\n",
    "                    if realvals[0] == 0:\n",
    "                        n = n - 1\n",
    "                        continue\n",
    "\n",
    "                    err = abs((realvals[0]-predvals[0])/realvals[0]) # find relative error\n",
    "\n",
    "\n",
    "                    if n == 0:\n",
    "                        mape = None\n",
    "                    else:\n",
    "                        mape = err # calculate mape\n",
    "\n",
    "\n",
    "                    data = {'Model': model,'Location': target_location, 'MAPE':mape}\n",
    "\n",
    "                    # store in pandas DataFrame\n",
    "                    newdf = pd.DataFrame(data, index=[1])\n",
    "\n",
    "                    mapedf = pd.concat([mapedf, newdf])\n",
    "\n",
    "        mapedf = mapedf.reset_index()\n",
    "        mapedf = mapedf.drop(['index'], axis=1)\n",
    "\n",
    "        return mapedf\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dcb7f3-a01b-4daf-877a-2d5c0e427fe8",
   "metadata": {},
   "source": [
    "# Calculate Scores\n",
    "\n",
    "## Instantiate Forecast_Eval Class and Format Data for Scoring\n",
    "\n",
    "I'm keeping the dates and models specified earlier."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5cbb9bf4-811b-4a3d-8822-46b42aa32b3f",
   "metadata": {},
   "source": [
    "### From Clara's notebook\n",
    "# dates of the forecast submission dates you want to score\n",
    "#dates = [ '2024-04-06', '2024-04-13', '2024-04-20', '2024-04-27']\n",
    "\n",
    "# models you want to score\n",
    "#models = ['CADPH-FluCAT_Ensemble', 'CEPH-Rtrend_fluH',  'CMU-TimeSeries', 'CU-ensemble', 'FluSight-baseline',\n",
    "#          'FluSight-ensemble','FluSight-equal_cat', 'FluSight-lop_norm', 'GH-model', 'GT-FluFNP', 'ISU_NiemiLab-ENS', \n",
    " #         'ISU_NiemiLab-NLH','ISU_NiemiLab-SIR', 'LUcompUncertLab-chimera', 'LosAlamos_NAU-CModel_Flu', \n",
    " #         'MIGHTE-Nsemble','MOBS-GLEAM_FLUH', 'NIH-Flu_ARIMA', 'PSI-PROF', 'SGroup-RandomForest', 'SigSci-CREG', \n",
    " #         'SigSci-TSENS','Stevens-GBR', 'UGA_flucast-Copycat', 'UGA_flucast-INFLAenza', 'UGA_flucast-OKeeffe', \n",
    " #         'UGuelph-CompositeCurve', 'UGuelphensemble-GRYPHON', 'UM-DeepOutbreak', 'UMass-flusion', \n",
    " #         'UMass-trends_ensemble', 'UNC_IDD-InfluPaint', 'UVAFluX-Ensemble', 'VTSanghani-Ensemble', 'cfa-flumech',\n",
    " #         'cfarenewal-cfaepimlight', 'fjordhest-ensemble', 'NU_UCSD-GLEAM_AI_FLUH', 'PSI-PROF_beta',\n",
    " #         'JHU_CSSE-CSSE_Ensemble', 'FluSight-national_cat', 'FluSight-ens_q_cat', 'FluSight-baseline_cat']\n",
    "\n",
    "# models to score\n",
    "#models = [ 'CEPH-Rtrend_fluH', 'FluSight-baseline', 'FluSight-ensemble','MIGHTE-Nsemble','MOBS-GLEAM_FLUH', \n",
    "#          'cfarenewal-cfaepimlight', 'NU_UCSD-GLEAM_AI_FLUH', 'JHU_CSSE-CSSE_Ensemble', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc316645-5c70-4a96-ad94-ce79afeb30e2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: './dat/CEPH-Rtrend_fluH_2023-08-12.pq'\n",
      "[Errno 2] No such file or directory: './dat/CEPH-Rtrend_fluH_2023-08-19.pq'\n",
      "[Errno 2] No such file or directory: './dat/CEPH-Rtrend_fluH_2023-08-26.pq'\n",
      "[Errno 2] No such file or directory: './dat/CEPH-Rtrend_fluH_2023-09-02.pq'\n",
      "[Errno 2] No such file or directory: './dat/CEPH-Rtrend_fluH_2023-09-09.pq'\n",
      "[Errno 2] No such file or directory: './dat/CEPH-Rtrend_fluH_2023-09-16.pq'\n",
      "[Errno 2] No such file or directory: './dat/CEPH-Rtrend_fluH_2023-09-23.pq'\n",
      "[Errno 2] No such file or directory: './dat/CEPH-Rtrend_fluH_2023-09-30.pq'\n",
      "[Errno 2] No such file or directory: './dat/CEPH-Rtrend_fluH_2023-10-07.pq'\n",
      "[Errno 2] No such file or directory: './dat/CEPH-Rtrend_fluH_2023-06-03.pq'\n",
      "[Errno 2] No such file or directory: './dat/CEPH-Rtrend_fluH_2023-06-10.pq'\n",
      "[Errno 2] No such file or directory: './dat/CEPH-Rtrend_fluH_2023-06-17.pq'\n",
      "[Errno 2] No such file or directory: './dat/CEPH-Rtrend_fluH_2023-06-24.pq'\n",
      "[Errno 2] No such file or directory: './dat/CEPH-Rtrend_fluH_2023-07-01.pq'\n",
      "[Errno 2] No such file or directory: './dat/CEPH-Rtrend_fluH_2023-07-08.pq'\n",
      "[Errno 2] No such file or directory: './dat/CEPH-Rtrend_fluH_2023-07-15.pq'\n",
      "[Errno 2] No such file or directory: './dat/CEPH-Rtrend_fluH_2023-07-22.pq'\n",
      "[Errno 2] No such file or directory: './dat/CEPH-Rtrend_fluH_2023-07-29.pq'\n",
      "[Errno 2] No such file or directory: './dat/CEPH-Rtrend_fluH_2023-08-05.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-baseline_2023-08-12.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-baseline_2023-08-19.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-baseline_2023-08-26.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-baseline_2023-09-02.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-baseline_2023-09-09.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-baseline_2023-09-16.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-baseline_2023-09-23.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-baseline_2023-09-30.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-baseline_2023-10-07.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-baseline_2023-06-03.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-baseline_2023-06-10.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-baseline_2023-06-17.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-baseline_2023-06-24.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-baseline_2023-07-01.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-baseline_2023-07-08.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-baseline_2023-07-15.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-baseline_2023-07-22.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-baseline_2023-07-29.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-baseline_2023-08-05.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-ensemble_2023-08-12.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-ensemble_2023-08-19.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-ensemble_2023-08-26.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-ensemble_2023-09-02.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-ensemble_2023-09-09.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-ensemble_2023-09-16.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-ensemble_2023-09-23.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-ensemble_2023-09-30.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-ensemble_2023-10-07.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-ensemble_2023-06-03.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-ensemble_2023-06-10.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-ensemble_2023-06-17.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-ensemble_2023-06-24.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-ensemble_2023-07-01.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-ensemble_2023-07-08.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-ensemble_2023-07-15.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-ensemble_2023-07-22.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-ensemble_2023-07-29.pq'\n",
      "[Errno 2] No such file or directory: './dat/FluSight-ensemble_2023-08-05.pq'\n",
      "[Errno 2] No such file or directory: './dat/MIGHTE-Nsemble_2023-08-12.pq'\n",
      "[Errno 2] No such file or directory: './dat/MIGHTE-Nsemble_2023-08-19.pq'\n",
      "[Errno 2] No such file or directory: './dat/MIGHTE-Nsemble_2023-08-26.pq'\n",
      "[Errno 2] No such file or directory: './dat/MIGHTE-Nsemble_2023-09-02.pq'\n",
      "[Errno 2] No such file or directory: './dat/MIGHTE-Nsemble_2023-09-09.pq'\n",
      "[Errno 2] No such file or directory: './dat/MIGHTE-Nsemble_2023-09-16.pq'\n",
      "[Errno 2] No such file or directory: './dat/MIGHTE-Nsemble_2023-09-23.pq'\n",
      "[Errno 2] No such file or directory: './dat/MIGHTE-Nsemble_2023-09-30.pq'\n",
      "[Errno 2] No such file or directory: './dat/MIGHTE-Nsemble_2023-10-07.pq'\n",
      "[Errno 2] No such file or directory: './dat/MIGHTE-Nsemble_2023-06-03.pq'\n",
      "[Errno 2] No such file or directory: './dat/MIGHTE-Nsemble_2023-06-10.pq'\n",
      "[Errno 2] No such file or directory: './dat/MIGHTE-Nsemble_2023-06-17.pq'\n",
      "[Errno 2] No such file or directory: './dat/MIGHTE-Nsemble_2023-06-24.pq'\n",
      "[Errno 2] No such file or directory: './dat/MIGHTE-Nsemble_2023-07-01.pq'\n",
      "[Errno 2] No such file or directory: './dat/MIGHTE-Nsemble_2023-07-08.pq'\n",
      "[Errno 2] No such file or directory: './dat/MIGHTE-Nsemble_2023-07-15.pq'\n",
      "[Errno 2] No such file or directory: './dat/MIGHTE-Nsemble_2023-07-22.pq'\n",
      "[Errno 2] No such file or directory: './dat/MIGHTE-Nsemble_2023-07-29.pq'\n",
      "[Errno 2] No such file or directory: './dat/MIGHTE-Nsemble_2023-08-05.pq'\n",
      "[Errno 2] No such file or directory: './dat/MOBS-GLEAM_FLUH_2023-08-12.pq'\n",
      "[Errno 2] No such file or directory: './dat/MOBS-GLEAM_FLUH_2023-08-19.pq'\n",
      "[Errno 2] No such file or directory: './dat/MOBS-GLEAM_FLUH_2023-08-26.pq'\n",
      "[Errno 2] No such file or directory: './dat/MOBS-GLEAM_FLUH_2023-09-02.pq'\n",
      "[Errno 2] No such file or directory: './dat/MOBS-GLEAM_FLUH_2023-09-09.pq'\n",
      "[Errno 2] No such file or directory: './dat/MOBS-GLEAM_FLUH_2023-09-16.pq'\n",
      "[Errno 2] No such file or directory: './dat/MOBS-GLEAM_FLUH_2023-09-23.pq'\n",
      "[Errno 2] No such file or directory: './dat/MOBS-GLEAM_FLUH_2023-09-30.pq'\n",
      "[Errno 2] No such file or directory: './dat/MOBS-GLEAM_FLUH_2023-10-07.pq'\n",
      "[Errno 2] No such file or directory: './dat/MOBS-GLEAM_FLUH_2023-06-03.pq'\n",
      "[Errno 2] No such file or directory: './dat/MOBS-GLEAM_FLUH_2023-06-10.pq'\n",
      "[Errno 2] No such file or directory: './dat/MOBS-GLEAM_FLUH_2023-06-17.pq'\n",
      "[Errno 2] No such file or directory: './dat/MOBS-GLEAM_FLUH_2023-06-24.pq'\n",
      "[Errno 2] No such file or directory: './dat/MOBS-GLEAM_FLUH_2023-07-01.pq'\n",
      "[Errno 2] No such file or directory: './dat/MOBS-GLEAM_FLUH_2023-07-08.pq'\n",
      "[Errno 2] No such file or directory: './dat/MOBS-GLEAM_FLUH_2023-07-15.pq'\n",
      "[Errno 2] No such file or directory: './dat/MOBS-GLEAM_FLUH_2023-07-22.pq'\n",
      "[Errno 2] No such file or directory: './dat/MOBS-GLEAM_FLUH_2023-07-29.pq'\n",
      "[Errno 2] No such file or directory: './dat/MOBS-GLEAM_FLUH_2023-08-05.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-08-12.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-08-19.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-08-26.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-09-02.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-09-09.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-09-16.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-09-23.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-09-30.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-10-07.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-10-14.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-10-21.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-10-28.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-11-04.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-11-11.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-11-18.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-11-25.pq'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-06-03.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-06-10.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-06-17.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-06-24.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-07-01.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-07-08.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-07-15.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-07-22.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-07-29.pq'\n",
      "[Errno 2] No such file or directory: './dat/NU_UCSD-GLEAM_AI_FLUH_2023-08-05.pq'\n"
     ]
    }
   ],
   "source": [
    "# put all forecasts into one dataframe\n",
    "predictionsall = pd.DataFrame()\n",
    "for model in models:\n",
    "    for date in dates:\n",
    "        try:\n",
    "            predictions = pd.read_parquet(f'./dat/{model}_{date}.pq')\n",
    "            predictions['Model'] = model\n",
    "            predictionsall = pd.concat([predictionsall, predictions])\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "becaa833-2311-4a09-b131-839785ad75dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format forecasts in order to calculate scores\n",
    "# input start and end weeks for the period of interest\n",
    "test = Forecast_Eval(df=pd.DataFrame(), obsdf=surv, target='hosp', \n",
    "                            start_week = Week(2023,40), end_week = Week(2024, 17))\n",
    "predsall = test.format_forecasts_all( dfformat = predictionsall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0ef5754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference_date</th>\n",
       "      <th>target</th>\n",
       "      <th>horizon</th>\n",
       "      <th>target_end_date</th>\n",
       "      <th>location</th>\n",
       "      <th>output_type</th>\n",
       "      <th>output_type_id</th>\n",
       "      <th>value</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-10-14</td>\n",
       "      <td>wk inc flu hosp</td>\n",
       "      <td>-1</td>\n",
       "      <td>2023-10-07</td>\n",
       "      <td>01</td>\n",
       "      <td>quantile</td>\n",
       "      <td>0.010</td>\n",
       "      <td>3.000</td>\n",
       "      <td>CEPH-Rtrend_fluH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-10-14</td>\n",
       "      <td>wk inc flu hosp</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-10-14</td>\n",
       "      <td>01</td>\n",
       "      <td>quantile</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>CEPH-Rtrend_fluH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-10-14</td>\n",
       "      <td>wk inc flu hosp</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-10-21</td>\n",
       "      <td>01</td>\n",
       "      <td>quantile</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>CEPH-Rtrend_fluH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-10-14</td>\n",
       "      <td>wk inc flu hosp</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-10-28</td>\n",
       "      <td>01</td>\n",
       "      <td>quantile</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>CEPH-Rtrend_fluH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-10-14</td>\n",
       "      <td>wk inc flu hosp</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-11-04</td>\n",
       "      <td>01</td>\n",
       "      <td>quantile</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>CEPH-Rtrend_fluH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5966</th>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>wk inc flu hosp</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>56</td>\n",
       "      <td>quantile</td>\n",
       "      <td>0.950</td>\n",
       "      <td>14.100</td>\n",
       "      <td>NU_UCSD-GLEAM_AI_FLUH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5970</th>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>wk inc flu hosp</td>\n",
       "      <td>-1</td>\n",
       "      <td>2024-04-20</td>\n",
       "      <td>56</td>\n",
       "      <td>quantile</td>\n",
       "      <td>0.975</td>\n",
       "      <td>24.489</td>\n",
       "      <td>NU_UCSD-GLEAM_AI_FLUH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5971</th>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>wk inc flu hosp</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>56</td>\n",
       "      <td>quantile</td>\n",
       "      <td>0.975</td>\n",
       "      <td>16.236</td>\n",
       "      <td>NU_UCSD-GLEAM_AI_FLUH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5975</th>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>wk inc flu hosp</td>\n",
       "      <td>-1</td>\n",
       "      <td>2024-04-20</td>\n",
       "      <td>56</td>\n",
       "      <td>quantile</td>\n",
       "      <td>0.990</td>\n",
       "      <td>29.184</td>\n",
       "      <td>NU_UCSD-GLEAM_AI_FLUH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5976</th>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>wk inc flu hosp</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>56</td>\n",
       "      <td>quantile</td>\n",
       "      <td>0.990</td>\n",
       "      <td>18.705</td>\n",
       "      <td>NU_UCSD-GLEAM_AI_FLUH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>936146 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     reference_date           target  horizon target_end_date location  \\\n",
       "0        2023-10-14  wk inc flu hosp       -1      2023-10-07       01   \n",
       "1        2023-10-14  wk inc flu hosp        0      2023-10-14       01   \n",
       "2        2023-10-14  wk inc flu hosp        1      2023-10-21       01   \n",
       "3        2023-10-14  wk inc flu hosp        2      2023-10-28       01   \n",
       "4        2023-10-14  wk inc flu hosp        3      2023-11-04       01   \n",
       "...             ...              ...      ...             ...      ...   \n",
       "5966     2024-04-27  wk inc flu hosp        0      2024-04-27       56   \n",
       "5970     2024-04-27  wk inc flu hosp       -1      2024-04-20       56   \n",
       "5971     2024-04-27  wk inc flu hosp        0      2024-04-27       56   \n",
       "5975     2024-04-27  wk inc flu hosp       -1      2024-04-20       56   \n",
       "5976     2024-04-27  wk inc flu hosp        0      2024-04-27       56   \n",
       "\n",
       "     output_type  output_type_id   value                  Model  \n",
       "0       quantile           0.010   3.000       CEPH-Rtrend_fluH  \n",
       "1       quantile           0.010   0.000       CEPH-Rtrend_fluH  \n",
       "2       quantile           0.010   0.000       CEPH-Rtrend_fluH  \n",
       "3       quantile           0.010   0.000       CEPH-Rtrend_fluH  \n",
       "4       quantile           0.010   0.000       CEPH-Rtrend_fluH  \n",
       "...          ...             ...     ...                    ...  \n",
       "5966    quantile           0.950  14.100  NU_UCSD-GLEAM_AI_FLUH  \n",
       "5970    quantile           0.975  24.489  NU_UCSD-GLEAM_AI_FLUH  \n",
       "5971    quantile           0.975  16.236  NU_UCSD-GLEAM_AI_FLUH  \n",
       "5975    quantile           0.990  29.184  NU_UCSD-GLEAM_AI_FLUH  \n",
       "5976    quantile           0.990  18.705  NU_UCSD-GLEAM_AI_FLUH  \n",
       "\n",
       "[936146 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predsall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d54454-0767-45d8-a775-c464d0722da8",
   "metadata": {},
   "source": [
    "## WIS\n",
    "\n",
    "At first I was working with a conda environment with fully updated packages. The following chunk threw an error in organize_timestamped_scores which traced down to something in Pandas that was making a deep copy. I changed my conda requirements to enforce python=3.10.* and pandas=1.4.* which eliminated the error. If we want to fix this to run on current versions of the required packages I can recreate the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a5a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfwis = pd.DataFrame()\n",
    "#dic = {}\n",
    "\n",
    "for horizon in [0, 1, 2,3]:\n",
    "    for model in models:\n",
    "        for date in dates: \n",
    "            for loc in predsall.location.unique():\n",
    "                start_week = Week.fromdate(pd.to_datetime(date)) # week of submission date\n",
    "                end_week = start_week + 3 # target end date of last horizon\n",
    "\n",
    "                # filter by horizon, model and submission date\n",
    "                pred = predsall[(predsall.horizon==horizon) & (predsall.Model == model) & \\\n",
    "                                (predsall.reference_date == date) & (predsall.location==loc)]\n",
    "\n",
    "                test = Scoring(df=pred, obsdf=surv, target='hosp')\n",
    "                predss = test.process_predictions(pred, t_col = 'target_end_date', quantile_col = 'output_type_id')\n",
    "\n",
    "                if len(predss)==0:\n",
    "                    continue\n",
    "\n",
    "                obs = test.get_observations(loc)\n",
    "                obs = obs[obs.date==pred.target_end_date.unique()[0]]\n",
    "                \n",
    "                out = test.timestamp_wis(obs, predss)\n",
    "\n",
    "                dfwis = pd.concat([dfwis, out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0033cdb-02d3-42b0-827e-91b2b1deab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfwis.to_csv('./WIS.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f028da7-0699-4d7d-b8ef-64e55b4377f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5ee3316-8513-4a97-ae00-623d05d0b606",
   "metadata": {},
   "source": [
    "## WIS Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdaa32c-25c0-413c-9f48-51a354682ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute wis ratio, comparing the Flusight models' forecast scores to the Flusight baseline model\n",
    "# divide flusight models by flusight baseline WIS scores at each location, week, horizon, location\n",
    "dfwis = pd.read_csv('./WIS.csv')\n",
    "baseline = dfwis[dfwis.Model == 'FluSight-baseline'] \n",
    "baseline = baseline.rename(columns={'wis':'wis_baseline', 'Model':'baseline'})\n",
    "dfwis_test = dfwis[dfwis.Model != 'FluSight-baseline']\n",
    "\n",
    "dfwis_ratio = pd.merge(dfwis_test, baseline, how='inner', on = ['location', 'target_end_date',\n",
    "                                                                'horizon', 'reference_date'])\n",
    "\n",
    "# calculate wis ratio\n",
    "dfwis_ratio['wis_ratio'] = dfwis_ratio['wis']/dfwis_ratio['wis_baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef766a4-de11-465c-8188-7bff9402806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfwis_ratio.to_csv('./WIS_ratio.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e13e46d-a6ef-4f18-8c0f-258cb38409e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfwis_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9210ecf5-de59-4228-a65d-ba10ba3c9962",
   "metadata": {},
   "source": [
    "## Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6771a6cc-0993-4180-8cf1-3a32389408b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLD\n",
    "dfcoverage = pd.DataFrame()\n",
    "\n",
    "for date in dates:\n",
    "    for model in models:\n",
    "         \n",
    "        start_week = Week.fromdate(pd.to_datetime(date)) # week of submission date\n",
    "        end_week = start_week + 3 # target end date of last horizon\n",
    "\n",
    "        # filter by model and submission date, only look at horizon 0-3\n",
    "        pred = predsall[(predsall.Model == model) & \\\n",
    "                        (predsall.reference_date == date) & (predsall.horizon >=0)]\n",
    "        if len(pred)==0:\n",
    "            continue\n",
    "\n",
    "        # calculate wis for each week\n",
    "        test = Scoring(df=pred, obsdf=surv, target='hosp',  \n",
    "                        start_week = start_week, end_week = end_week)\n",
    "\n",
    "        out = test.organize_average_scores(want_scores=['10_cov', '20_cov', '30_cov', '40_cov', '50_cov',\n",
    "            '60_cov', '70_cov', '80_cov', '90_cov', '95_cov', '98_cov'], models = [model])\n",
    "\n",
    "        out['horizon'] = horizon\n",
    "        out['reference_date'] = date\n",
    "\n",
    "        dfcoverage = pd.concat([dfcoverage, out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e1f2470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcoverage = pd.DataFrame()\n",
    "\n",
    "for date in dates:\n",
    "    for model in models:\n",
    "        for loc in predsall.location.unique():\n",
    "            for horizon in [0,1,2,3]:\n",
    "                start_week = Week.fromdate(pd.to_datetime(date)) # week of submission date\n",
    "                end_week = start_week + 3 # target end date of last horizon\n",
    "\n",
    "                # filter by model and submission date, only look at horizon 0-3\n",
    "                pred = predsall[(predsall.Model == model)& (predsall.reference_date == date) &\\\n",
    "                                (predsall.horizon ==horizon) & (predsall.location==loc)]\n",
    "\n",
    "                if len(pred)==0:\n",
    "                    continue\n",
    "\n",
    "                test = Scoring(df=pred, obsdf=surv, target='hosp')\n",
    "                predss = test.process_predictions(pred, t_col = 'target_end_date', quantile_col = 'output_type_id')\n",
    "\n",
    "\n",
    "                obs = test.get_observations(loc)\n",
    "                obs = obs[obs.date.isin(pred.target_end_date.unique())]\n",
    "\n",
    "                out = test.all_coverages_from_df(obs, predss)\n",
    "\n",
    "                out['horizon'] = horizon\n",
    "                out['Model'] = model\n",
    "                out['reference_date'] = date\n",
    "                out['location'] = loc\n",
    "\n",
    "                dfcoverage = pd.concat([dfcoverage, pd.DataFrame(out,index=[0])])\n",
    "dfcoverage = dfcoverage.reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6d2e5ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10_cov</th>\n",
       "      <th>20_cov</th>\n",
       "      <th>30_cov</th>\n",
       "      <th>40_cov</th>\n",
       "      <th>50_cov</th>\n",
       "      <th>60_cov</th>\n",
       "      <th>70_cov</th>\n",
       "      <th>80_cov</th>\n",
       "      <th>90_cov</th>\n",
       "      <th>95_cov</th>\n",
       "      <th>98_cov</th>\n",
       "      <th>horizon</th>\n",
       "      <th>Model</th>\n",
       "      <th>reference_date</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>CEPH-Rtrend_fluH</td>\n",
       "      <td>2023-10-14</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>CEPH-Rtrend_fluH</td>\n",
       "      <td>2023-10-14</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>CEPH-Rtrend_fluH</td>\n",
       "      <td>2023-10-14</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>CEPH-Rtrend_fluH</td>\n",
       "      <td>2023-10-14</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>CEPH-Rtrend_fluH</td>\n",
       "      <td>2023-10-14</td>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33227</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NU_UCSD-GLEAM_AI_FLUH</td>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33228</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NU_UCSD-GLEAM_AI_FLUH</td>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33229</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NU_UCSD-GLEAM_AI_FLUH</td>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33230</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NU_UCSD-GLEAM_AI_FLUH</td>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33231</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NU_UCSD-GLEAM_AI_FLUH</td>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33232 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       10_cov  20_cov  30_cov  40_cov  50_cov  60_cov  70_cov  80_cov  90_cov  \\\n",
       "0         1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0   \n",
       "1         1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0   \n",
       "2         1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0   \n",
       "3         1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0   \n",
       "4         0.0     0.0     0.0     0.0     0.0     1.0     1.0     1.0     1.0   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "33227     0.0     0.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0   \n",
       "33228     0.0     0.0     0.0     1.0     1.0     1.0     1.0     1.0     1.0   \n",
       "33229     0.0     0.0     0.0     0.0     1.0     1.0     1.0     1.0     1.0   \n",
       "33230     0.0     0.0     0.0     1.0     1.0     1.0     1.0     1.0     1.0   \n",
       "33231     0.0     0.0     1.0     1.0     1.0     1.0     1.0     1.0     1.0   \n",
       "\n",
       "       95_cov  98_cov  horizon                  Model reference_date location  \n",
       "0         1.0     1.0        0       CEPH-Rtrend_fluH     2023-10-14       01  \n",
       "1         1.0     1.0        1       CEPH-Rtrend_fluH     2023-10-14       01  \n",
       "2         1.0     1.0        2       CEPH-Rtrend_fluH     2023-10-14       01  \n",
       "3         1.0     1.0        3       CEPH-Rtrend_fluH     2023-10-14       01  \n",
       "4         1.0     1.0        0       CEPH-Rtrend_fluH     2023-10-14       02  \n",
       "...       ...     ...      ...                    ...            ...      ...  \n",
       "33227     1.0     1.0        0  NU_UCSD-GLEAM_AI_FLUH     2024-04-27       53  \n",
       "33228     1.0     1.0        0  NU_UCSD-GLEAM_AI_FLUH     2024-04-27       54  \n",
       "33229     1.0     1.0        0  NU_UCSD-GLEAM_AI_FLUH     2024-04-27       55  \n",
       "33230     1.0     1.0        0  NU_UCSD-GLEAM_AI_FLUH     2024-04-27       56  \n",
       "33231     1.0     1.0        0  NU_UCSD-GLEAM_AI_FLUH     2024-04-27       US  \n",
       "\n",
       "[33232 rows x 15 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcoverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4e655b66-10f5-436a-b87e-f91e681ad909",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcoverage.to_csv('./coverage.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bab22c",
   "metadata": {},
   "source": [
    "# MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5fb010",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate MAPE for all forecasts\n",
    "\n",
    "dfmape = pd.DataFrame()\n",
    "#dic = {}\n",
    "for horizon in [0, 1, 2,3]:\n",
    "    for model in models:\n",
    "        for date in dates: \n",
    "            start_week = Week.fromdate(pd.to_datetime(date)) # week of submission date\n",
    "            end_week = start_week + 3 # target end date of last horizon\n",
    "            \n",
    "            # filter by horizon, model and submission date\n",
    "            pred = predsall[(predsall.horizon==horizon) & (predsall.Model == model) & \\\n",
    "                            (predsall.reference_date == date)]\n",
    "            if len(pred)==0:\n",
    "                continue\n",
    "            \n",
    "            # calculate mape for each week\n",
    "            test = Scoring(df=pred, obsdf=surv, target='hosp',\n",
    "                            start_week = start_week, end_week = end_week)\n",
    "\n",
    "            out = test.get_mape()\n",
    "            \n",
    "            out['horizon'] = horizon\n",
    "            out['reference_date'] = date\n",
    "            \n",
    "            dfmape = pd.concat([dfmape, out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb310c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmape.to_csv('./MAPE.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c04295",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f72c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
