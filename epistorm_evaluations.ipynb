{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53044182-bf0b-4dcd-b811-eee167b40de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocess as mp\n",
    "import datetime\n",
    "import itertools\n",
    "from epiweeks import Week\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# all existing models as of Dec 2024\n",
    "all_models = ['CADPH-FluCAT_Ensemble', 'CEPH-Rtrend_fluH',  'CMU-TimeSeries', 'CU-ensemble', 'FluSight-baseline',\n",
    "          'FluSight-ensemble','FluSight-equal_cat', 'FluSight-lop_norm', 'GH-model', 'GT-FluFNP', 'ISU_NiemiLab-ENS', \n",
    "          'ISU_NiemiLab-NLH','ISU_NiemiLab-SIR', 'LUcompUncertLab-chimera', 'LosAlamos_NAU-CModel_Flu', \n",
    "          'MIGHTE-Nsemble','MOBS-GLEAM_FLUH', 'NIH-Flu_ARIMA', 'PSI-PROF', 'SGroup-RandomForest', 'SigSci-CREG', \n",
    "          'SigSci-TSENS','Stevens-GBR', 'UGA_flucast-Copycat', 'UGA_flucast-INFLAenza', 'UGA_flucast-OKeeffe', \n",
    "          'UGuelph-CompositeCurve', 'UGuelphensemble-GRYPHON', 'UM-DeepOutbreak', 'UMass-flusion', 'UMass-trends_ensemble',\n",
    "          'UNC_IDD-InfluPaint', 'UVAFluX-Ensemble', 'VTSanghani-Ensemble', 'cfa-flumech', 'cfarenewal-cfaepimlight', \n",
    "          'fjordhest-ensemble', 'NU_UCSD-GLEAM_AI_FLUH', 'PSI-PROF_beta', 'JHU_CSSE-CSSE_Ensemble', 'FluSight-national_cat',\n",
    "          'FluSight-ens_q_cat', 'FluSight-baseline_cat', 'FluSight-base_seasonal', 'Gatech-ensemble_point', 'Gatech-ensemble_prob',\n",
    "          'ISU_NiemiLab-GPE', 'JHUAPL-DMD', 'MDPredict-SIRS', 'MIGHTE-Joint', 'Metaculus-cp', 'NEU_ISI-AdaptiveEnsemble',\n",
    "          'NEU_ISI-FluBcast', 'OHT_JHU-nbxd', 'SigSci-BECAM', 'Stevens-ILIForecast', 'UGA_CEID-Walk', 'UGA_flucast-Scenariocast',\n",
    "          'UI_CompEpi-EpiGen', 'UMass-AR2', 'VTSanghani-PRIME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abee3260-8ce9-42f4-9e73-5a8f0ec5c166",
   "metadata": {},
   "source": [
    "## Flu Forecast Evaluations\n",
    "\n",
    "This notebook reads from the `FluSight-forecast-hub` repo submodule. To ensure use of the latest data, run `git submodule update --remote` and `git submodule foreach git pull origin main` before running this notebook.\n",
    "\n",
    "Specify inputs below and run all cells. Outputs will be generated to the `scratch/` directory.\n",
    "\n",
    "# Inputs\n",
    "\n",
    "To evaluate all models, set `models = all_models`. To evaluate all dates, set `dates = 'all'`.\n",
    "\n",
    "To evaluate specific models, pass model names as a list of strings.\n",
    "\n",
    "To evaluate specific dates, pass in YYYY-MM-DD format as a list of strings e.g. `['1970-01-01', '2038-01-19']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b35414-4021-4ed5-a40e-85e1645110fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = all_models\n",
    "dates = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390336aa-aa4d-49f1-8b91-2d2499e92373",
   "metadata": {},
   "source": [
    "## Classes and Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f6c49-51b2-468e-91d7-d31c8940273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for calculating scores of the forecasts against the surveillance data\n",
    "class Forecast_Eval:\n",
    "    \"\"\" Used for scoring and evaluating flu forecasting predictions from the Flusight challenge for the \n",
    "        2023-24 season\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, obsdf, target,  start_week = False, end_week = False):\n",
    "        self.df = df # pandas Dataframe: input dataframe of forecasts with all scenarios, locations, and quantiles\n",
    "        self.obsdf = obsdf # pandas Dataframe: input of surveillance data of interest\n",
    "        self.target = target # str: target metric of interest (case, death, hospitalization)\n",
    "        self.start_week = start_week # epiweek: beginning of observations of interest\n",
    "        self.end_week = end_week # epiweek: end of observations of interest\n",
    "        \n",
    "    def process_observations(self, data, value_col='value', t_col='date', other_ind_cols=None):\n",
    "        \"\"\"\n",
    "    \n",
    "        Parameters:\n",
    "        - data: pd.DataFrame\n",
    "            The input data containing observational information.\n",
    "        - value_col: str\n",
    "            The column name representing the value column.\n",
    "        - t_col: str\n",
    "            The column name representing the time column.\n",
    "        - other_ind_cols: list\n",
    "            A list of additional independent columns for sorting.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame with additional helper functions for accessing specific columns.\n",
    "        \"\"\"\n",
    "        # Ensure required columns are present\n",
    "        if value_col not in data.columns or t_col not in data.columns:\n",
    "            raise ValueError(f\"DataFrame must contain '{value_col}' and '{t_col}' columns.\")\n",
    "\n",
    "        # Prepare independent columns and sort data\n",
    "        ind_cols = [t_col] + (other_ind_cols if other_ind_cols else [])\n",
    "        sorted_data = data.sort_values(by=ind_cols).reset_index(drop=True)\n",
    "\n",
    "        # Define helper functions to access specific columns and set them as attributes\n",
    "        sorted_data.get_value = lambda: sorted_data[value_col].to_numpy()\n",
    "        sorted_data.get_t = lambda: sorted_data[t_col].to_numpy()\n",
    "        sorted_data.get_x = lambda: sorted_data[ind_cols].to_numpy()\n",
    "        sorted_data.get_unique_x = lambda: np.unique(np.array(sorted_data[ind_cols].to_numpy(), dtype=str), axis=0)\n",
    "\n",
    "        return sorted_data      \n",
    "            \n",
    "    def get_observations(self, target_location):\n",
    "        \"\"\" get_observations. Load and filter surveillance data for a certain location.\n",
    "        Parameters\n",
    "        ----------\n",
    "        target_location : str\n",
    "            location to filter surveillance data by\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.target == 'hosp':\n",
    "            target_obs = 'hospitalization'\n",
    "        else:\n",
    "            target_obs = self.target\n",
    "            \n",
    "        # read in observations dataframe\n",
    "        \n",
    "        observations = self.obsdf.copy().drop(columns= ['Unnamed: 0', 'weekly_rate'])\n",
    "        observations['date'] = pd.to_datetime(observations['date'])\n",
    "\n",
    "        #filter start - end week\n",
    "        if self.start_week:\n",
    "            observations = observations[(observations['date'] >= pd.to_datetime(self.start_week.startdate())) ]\n",
    "            \n",
    "        if self.end_week:\n",
    "            observations = observations[(observations['date'] <= pd.to_datetime(self.end_week.enddate()))]\n",
    "                                \n",
    "        #filter location\n",
    "        observations = observations[observations['location'] == target_location]\n",
    "\n",
    "        #aggregate to weekly\n",
    "        observations = observations.groupby(['location', pd.Grouper(key='date', freq='W-SAT')]).sum().reset_index()\n",
    "\n",
    "        #transform to Observation object\n",
    "        observations = self.process_observations(observations)\n",
    "\n",
    "        return observations\n",
    "    \n",
    "    def process_predictions(self, data, value_col='value', quantile_col='output_type_id', type_col='output_type',\n",
    "                        t_col='target_end_date', other_ind_cols=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters:\n",
    "        - data: pd.DataFrame\n",
    "            The input data containing prediction information.\n",
    "        - value_col: str\n",
    "            Column label for the predictions' value.\n",
    "        - quantile_col: str\n",
    "            Column label for the predictions' quantile.\n",
    "        - type_col: str\n",
    "            Column label for the type of predictions (e.g., quantile or point).\n",
    "        - t_col: str\n",
    "            Column label for the timestamp of predictions.\n",
    "        - other_ind_cols: list\n",
    "            List of other independent variable columns (e.g., location).\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame with additional helper methods to access specific data arrays or filtered predictions.\n",
    "        \"\"\"\n",
    "        # Ensure required columns are present\n",
    "        if not all(col in data.columns for col in [value_col, quantile_col, t_col]):\n",
    "            raise ValueError(f\"DataFrame must contain '{value_col}', '{quantile_col}', and '{t_col}' columns.\")\n",
    "        if other_ind_cols and not all(col in data.columns for col in other_ind_cols):\n",
    "            raise ValueError(\"DataFrame must contain all specified independent columns.\")\n",
    "\n",
    "        # Define independent columns and sort data\n",
    "        ind_cols = [t_col] + (other_ind_cols if other_ind_cols else [])\n",
    "        sorted_data = data.sort_values(by=ind_cols).reset_index(drop=True)\n",
    "\n",
    "        # Set default value for type column if missing\n",
    "        if type_col not in sorted_data.columns:\n",
    "            sorted_data[type_col] = 'quantile'\n",
    "\n",
    "        # Attach helper methods as attributes of the DataFrame\n",
    "        sorted_data.get_t = lambda: sorted_data[t_col].to_numpy()\n",
    "        sorted_data.get_x = lambda: sorted_data[ind_cols].to_numpy()\n",
    "        sorted_data.get_unique_x = lambda: np.unique(np.array(sorted_data[ind_cols].to_numpy(), dtype=str), axis=0)\n",
    "\n",
    "        return sorted_data\n",
    "\n",
    "    def format_forecasts_all(self, dfformat):\n",
    "        \"\"\" format_forecasts_all. Get forecasts into standard format to use for scoring.\n",
    "        Parameters\n",
    "        ----------\n",
    "        dfformat : pandas DataFrame\n",
    "            dataframe of forecast output used for formatting\n",
    "        \"\"\"\n",
    "        \n",
    "        pred = dfformat.copy()\n",
    "        pred = pred[pred.output_type == 'quantile'] # only keep quantile predictions\n",
    "        pred['target_end_date'] = pd.to_datetime(pred['target_end_date']) #make sure dates are in datetime format\n",
    "        if self.start_week:\n",
    "            pred = pred[(pred['target_end_date'] >= pd.to_datetime(self.start_week.startdate()))] # filter dates\n",
    "        \n",
    "        if self.end_week:\n",
    "            pred = pred[(pred['target_end_date'] <= pd.to_datetime(self.end_week.enddate()))] # filter dates\n",
    "        \n",
    "        pred['output_type_id'] = pred[\"output_type_id\"].astype(float) # make sure quantile levels are floats\n",
    "        \n",
    "        return pred\n",
    "        \n",
    "    \n",
    "class Scoring(Forecast_Eval):\n",
    "    \"\"\" calculate score values for probabilistic epidemic forecasts \n",
    "    find WIS, MAPE, and coverage over whole projection window as well as timestamped for every week.\n",
    "    score dataframe must have 'Model' column to differentiate and calculate scores for different models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, obsdf, target, start_week = False, \n",
    "                 end_week = False):\n",
    "        super().__init__(df, obsdf, target, start_week, end_week)\n",
    "        \n",
    "    def interval_score(self, observation, lower, upper, interval_range, specify_range_out=False):\n",
    "        \"\"\"interval_score.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : array_like\n",
    "            Vector of observations.\n",
    "        lower : array_like\n",
    "            Prediction for the lower quantile.\n",
    "        upper : array_like\n",
    "            Prediction for the upper quantile.\n",
    "        interval_range : int\n",
    "            Percentage covered by the interval. For instance, if lower and upper correspond to 0.05 and 0.95\n",
    "            quantiles, interval_range is 90.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out : dict\n",
    "            Dictionary containing vectors for the interval scores, but also the dispersion, underprediction and\n",
    "            overprediction.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError:\n",
    "            If the observation, the lower and upper vectors are not the same length or if interval_range is not\n",
    "            between 0 and 100\n",
    "        \"\"\"\n",
    "        if len(lower) != len(upper) or len(lower) != len(observation):\n",
    "            raise ValueError(\"vector shape mismatch\")\n",
    "        if interval_range > 100 or interval_range < 0:\n",
    "            raise ValueError(\"interval range should be between 0 and 100\")\n",
    "\n",
    "        #make sure vector operation works\n",
    "        obs,l,u = np.array(observation),np.array(lower),np.array(upper)\n",
    "\n",
    "        alpha = 1-interval_range/100 #prediction probability outside the interval\n",
    "        dispersion = u - l\n",
    "        underprediction = (2/alpha) * (l-obs) * (obs < l)\n",
    "        overprediction = (2/alpha) * (obs-u) * (obs > u)\n",
    "        score = dispersion + underprediction + overprediction\n",
    "        if not specify_range_out:\n",
    "            out = {'interval_score': score,\n",
    "                   'dispersion': dispersion,\n",
    "                   'underprediction': underprediction,\n",
    "                   'overprediction': overprediction}\n",
    "        else:\n",
    "            out = {f'{interval_range}_interval_score': score,\n",
    "                   f'{interval_range}_dispersion': dispersion,\n",
    "                   f'{interval_range}_underprediction': underprediction,\n",
    "                   f'{interval_range}_overprediction': overprediction}\n",
    "        return out\n",
    "\n",
    "    def timestamp_wis(self,observations, predsfilt, interval_ranges =[10,20,30,40,50,60,70,80,90,95,98]):\n",
    "        \"\"\"timestamp_wis.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observations : Observations object\n",
    "            Specialized dateframe for the observations across time.\n",
    "        predictions : Predictions object\n",
    "            Specialized dateframe for the predictions (quantile and point) across time.\n",
    "        interval_ranges : list of int\n",
    "            Percentage covered by each interval. For instance, if interval_range is 90, this corresponds\n",
    "            to the interval for the 0.05 and 0.95 quantiles.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df : DataFrame\n",
    "            DataFrame containing the weighted interval score across time.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError:\n",
    "            If the independent columns do not match for observations and predictions.\n",
    "            If the median is not calculated.\n",
    "            If the point estimate is not included.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        quantiles = np.array(predsfilt.sort_values(by='output_type_id').output_type_id)\n",
    "\n",
    "        qs = []\n",
    "        for q in quantiles:\n",
    "            df = predsfilt[predsfilt.output_type_id==q].sort_values(by='target_end_date')\n",
    "            val = np.array(df.value)\n",
    "            qs.append(val)\n",
    "\n",
    "        Q = np.array(qs) # quantiles array\n",
    "        y = np.array(observations.value) # observations array\n",
    "\n",
    "        # calculate WIS\n",
    "        WIS = np.zeros(len(y))\n",
    "\n",
    "        for i in range(len(quantiles) // 2):\n",
    "            interval_range = 100*(quantiles[-i-1]-quantiles[i])\n",
    "            #print(interval_range)\n",
    "            alpha = 1-(quantiles[-i-1]-quantiles[i])\n",
    "            IS = self.interval_score(y,Q[i],Q[-i-1],interval_range)\n",
    "            WIS += IS['interval_score']*alpha/2\n",
    "        WIS += 0.5*np.abs(Q[11] - y)\n",
    "\n",
    "        WISlist = np.array(WIS) / (len(interval_ranges) + 0.5)\n",
    "\n",
    "        df = pd.DataFrame({'Model':predsfilt.Model.unique(), 'location':predsfilt.location.unique(), 'horizon':predsfilt.horizon.unique(),\n",
    "                           'reference_date':predsfilt.reference_date.unique(), 'target_end_date':predsfilt.target_end_date.unique(),\n",
    "                           'wis':WISlist[0]},index=[0])\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def coverage(self,observation,lower,upper):\n",
    "        \"\"\"coverage. Output the fraction of observations within lower and upper.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : array_like\n",
    "            Vector of observations.\n",
    "        lower : array_like\n",
    "            Prediction for the lower quantile.\n",
    "        upper : array_like\n",
    "            Prediction for the upper quantile.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cov : float\n",
    "            Fraction of observations within the lower and upper bound.\n",
    "\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError:\n",
    "            If the observation, the lower and upper vectors are not the same length.\n",
    "        \"\"\"\n",
    "        if len(lower) != len(upper) or len(lower) != len(observation):\n",
    "            raise ValueError(\"vector shape mismatch\")\n",
    "\n",
    "        #make sure vector operation works\n",
    "        obs,l,u = np.array(observation),np.array(lower),np.array(upper)\n",
    "\n",
    "        return np.mean(np.logical_and(obs >= l, obs <= u))\n",
    "\n",
    "    def all_coverages_from_df(self,observations, predictions, interval_ranges=[10,20,30,40,50,60,70,80,90,95,98],\n",
    "                              **kwargs):\n",
    "        \"\"\"all_coverages_from_df.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observations : DataFrame object\n",
    "            Dateframe for the observations across time.\n",
    "        predictions : DataFrame object\n",
    "            Dateframe for the predictions (intervals) across time.\n",
    "        interval_ranges : list of int\n",
    "            Percentage covered by each interval. For instance, if interval_range is 90, this corresponds\n",
    "            to the interval for the 0.05 and 0.95 quantiles.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out : dict\n",
    "            Dictionary containing the coverage for all interval ranges.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError:\n",
    "            If the independent columns do not match for observations and predictions.\n",
    "        \"\"\"\n",
    "        #verify that the independent variable columns (usually dates and location) matches\n",
    "        if not np.array_equal(observations.get_unique_x(), predictions.get_unique_x()):\n",
    "            raise ValueError(\"Values for the independent columns do not match\")\n",
    "\n",
    "        out = dict()\n",
    "        for interval_range in interval_ranges:\n",
    "            q_low,q_upp = round(0.5-interval_range/200,3),round(0.5+interval_range/200,3)\n",
    "            cov = self.coverage(list(observations.value),\n",
    "                           list(predictions[predictions.output_type_id ==q_low].value),\n",
    "                           list(predictions[predictions.output_type_id==q_upp].value))\n",
    "            out[f'{interval_range}_cov'] = cov\n",
    "        return out\n",
    "        \n",
    "    def get_mape(self):\n",
    "        \"\"\" get_mape. Calculate MAPE (mean absolute percentage error) for each date of a forecast. If \n",
    "            surveillance data point is equal to zero, the score is undefined (Nan).\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        predictions = self.df.copy()\n",
    "        \n",
    "        # get point forecast, here we say it is the median\n",
    "        predictions = predictions[predictions['output_type_id'] == 0.5] # get point forecast, here we say it is the median\n",
    "\n",
    "        mapedf = pd.DataFrame()\n",
    "\n",
    "        # find mape for a given model and location over projection period\n",
    "        for model in predictions.Model.unique():\n",
    "            for target_location in predictions.location.unique():\n",
    "\n",
    "                    if target_location in ['60','66','69', '72', '78']:\n",
    "                        continue\n",
    "\n",
    "                    observations = self.get_observations(target_location)\n",
    "                    \n",
    "                    pred = predictions[(predictions.location == target_location) & (predictions.Model==model)]\n",
    "                    pred = self.process_predictions(pred, t_col = 'target_end_date',quantile_col='output_type_id')\n",
    "                    \n",
    "                    observations = observations[observations.date.isin(pred.target_end_date.unique())]\n",
    "\n",
    "                    n = observations.shape[0]\n",
    "\n",
    "                    realvals = list(observations.value)\n",
    "                    predvals = list(pred.value)\n",
    "                    \n",
    "                    if len(predvals) == 0 or len(realvals) == 0: continue\n",
    "\n",
    "                    if realvals[0] == 0:\n",
    "                        n = n - 1\n",
    "                        continue\n",
    "\n",
    "                    err = abs((realvals[0]-predvals[0])/realvals[0]) # find relative error\n",
    "\n",
    "                    if n == 0:\n",
    "                        mape = None\n",
    "                    else:\n",
    "                        mape = err # calculate mape\n",
    "\n",
    "                    data = {'Model': model,'Location': target_location, 'MAPE':mape}\n",
    "\n",
    "                    # store in pandas DataFrame\n",
    "                    newdf = pd.DataFrame(data, index=[1])\n",
    "\n",
    "                    mapedf = pd.concat([mapedf, newdf])\n",
    "\n",
    "        mapedf = mapedf.reset_index()\n",
    "        mapedf = mapedf.drop(['index'], axis=1)\n",
    "\n",
    "        return mapedf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bdd4a5-3e62-4f12-925e-3f82010201bd",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d87cf3-fbb3-46b9-83e3-39e741cab4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report available and used RAM\n",
    "def report_memory():\n",
    "    '''\n",
    "    Report available and used RAM\n",
    "    Prints with flush=True\n",
    "    '''\n",
    "    import os\n",
    "\n",
    "    # Getting all memory using os.popen()\n",
    "    total_memory, used_memory, free_memory = map(\n",
    "\t    int, os.popen('free -t -m').readlines()[-1].split()[1:])\n",
    "\n",
    "    # Memory usage\n",
    "    print(f'Total RAM: {total_memory}\\nRAM % used: {round((used_memory/total_memory) * 100, 2)}', flush=True)\n",
    "\n",
    "report_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c375b95-13e5-44d1-88b2-e742223d3849",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reading data...')\n",
    "\n",
    "# read files for specified models and dates directly from the flusight repo folder\n",
    "surv = pd.read_csv('./FluSight-forecast-hub/target-data/target-hospital-admissions.csv')\n",
    "if dates == 'all': dates = pd.unique(surv.date)\n",
    "\n",
    "# ensure baseline is present, needed for WIS ratio\n",
    "models = set(models)\n",
    "models.add('FluSight-baseline')\n",
    "models = list(models)\n",
    "\n",
    "# read files\n",
    "predictionsall = pd.DataFrame()\n",
    "\n",
    "def read_preds_csv(model, date, ext):\n",
    "    '''\n",
    "    Read csv predictions in scratch mode.\n",
    "    Reads directly from FluSight repo.\n",
    "    '''\n",
    "    try:\n",
    "        predictions = pd.read_csv(f'./FluSight-forecast-hub/model-output/{model}/{date}-{model}{ext}', dtype={'location':object})\n",
    "        predictions['Model'] = model\n",
    "        return predictions\n",
    "    except Exception:\n",
    "        return\n",
    "    \n",
    "def read_preds_pq(model, date, ext):\n",
    "    '''\n",
    "    Read parquet predictions in scratch mode.\n",
    "    Reads directly from FluSight repo.\n",
    "    '''\n",
    "    try:\n",
    "        predictions = pd.read_parquet(f'./FluSight-forecast-hub/model-output/{model}/{date}-{model}{ext}')\n",
    "        predictions['Model'] = model\n",
    "        return predictions\n",
    "    except Exception:\n",
    "        return\n",
    "    \n",
    "with mp.Pool() as pool:\n",
    "    import os\n",
    "    print(f'{len(os.sched_getaffinity(0))} cores available', flush=True)\n",
    "    \n",
    "    a = [models, dates, [\".csv\",\".gz\",\".zip\",\".csv.zip\",\".csv.gz\"]]\n",
    "    arguments = list(itertools.product(*a))\n",
    "    try:\n",
    "        preds = pd.concat(pool.starmap(read_preds_csv, arguments))\n",
    "        predictionsall = pd.concat([predictionsall, preds]).drop_duplicates().reset_index(drop=True)\n",
    "    except ValueError as e:\n",
    "        print(f'{e}\\nIf error \\\"All objects passed were None\\\" no csv files found', flush=True)\n",
    "    \n",
    "    a = [models, dates, ['.parquet','.pq',\".gz\",\".zip\"]]\n",
    "    arguments = list(itertools.product(*a))\n",
    "    try:\n",
    "        preds = pd.concat(pool.starmap(read_preds_pq, arguments))\n",
    "        predictionsall = pd.concat([predictionsall, preds]).drop_duplicates().reset_index(drop=True)\n",
    "    except ValueError as e:\n",
    "        print(f'{e}\\nIf error \\\"All objects passed were None\\\" no parquet files found', flush=True)\n",
    "                \n",
    "\n",
    "print('Data reading completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f98aae-b3a7-4eed-8006-a91e13b814e4",
   "metadata": {},
   "source": [
    "## Calculate Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cad4e3-8364-4c4c-81aa-34111068d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Forecast_Eval Class and Format Data for Scoring\n",
    "surv['Unnamed: 0'] = 0 # needed for Forecast_Eval methods\n",
    "test = Forecast_Eval(df=pd.DataFrame(), obsdf=surv, target='hosp')\n",
    "predsall = test.format_forecasts_all(dfformat = predictionsall)\n",
    "del predictionsall\n",
    "print(f'Predictions to score:\\n{predsall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ee1ce-5c92-4a6f-91c5-5f2da43bc68c",
   "metadata": {},
   "source": [
    "### WIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1b5fd-9ea5-45bb-8960-9a3eeddbee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate WIS for all forecasts\n",
    "print('Calculating WIS...')\n",
    "\n",
    "def batch_wis(model, date, loc, horizon, verbose=False):\n",
    "    '''\n",
    "    Calculate WIS for given model, date, location, and horizon.\n",
    "    Assumes Scoring class is defined and formatted predsall dataframe is available.\n",
    "    \n",
    "    Arguments:\n",
    "      model   - model name as string\n",
    "      date    - reference date as formatted in predsall\n",
    "      loc     - location code as string\n",
    "      horizon - horizon as int\n",
    "      verbose - if True prints message if data exist and scoring is completed\n",
    "    \n",
    "    Returns:\n",
    "      formatted dataframe of scores\n",
    "    '''\n",
    "    # filter by horizon, model and submission date\n",
    "    pred = predsall[(predsall.horizon==horizon) & (predsall.Model == model) & \\\n",
    "                    (predsall.reference_date == date) & (predsall.location==loc)]\n",
    "\n",
    "    test = Scoring(df=pred, obsdf=surv, target='hosp')\n",
    "    predss = test.process_predictions(pred, t_col = 'target_end_date', quantile_col = 'output_type_id')\n",
    "\n",
    "    if len(predss) == 0: return\n",
    "\n",
    "    obs = test.get_observations(loc)\n",
    "    obs = obs[obs.date==pred.target_end_date.unique()[0]]\n",
    "\n",
    "    if len(obs) == 0: return\n",
    "\n",
    "    out = test.timestamp_wis(obs, predss)\n",
    "    \n",
    "    if verbose: print(f'WIS completed {model} {date} location {loc} horizon {horizon}', flush=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "dfwis = pd.DataFrame()\n",
    "with mp.Pool() as pool:\n",
    "    import os\n",
    "    print(f'{len(os.sched_getaffinity(0))} cores available', flush=True)\n",
    "    report_memory()\n",
    "    arguments = set(_ for _ in predsall[['Model','reference_date','location','horizon']].itertuples(index=False, name=None))\n",
    "    scores = pool.starmap(batch_wis, arguments)\n",
    "    dfwis = pd.concat(scores)\n",
    "\n",
    "# save to csv\n",
    "print('Saving WIS to file...')\n",
    "dfwis.to_csv('./scratch/WIS.csv', index=False, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5f7074-72ef-4c26-8a9c-e40ca016dd32",
   "metadata": {},
   "source": [
    "### WIS Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41619f16-dd9b-453a-8d79-c35fe34b93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute wis ratio, comparing the Flusight models' forecast scores to the Flusight baseline model\n",
    "# divide flusight models by flusight baseline WIS scores at each location, week, horizon, location\n",
    "print('Calculating WIS ratio...')\n",
    "baseline = dfwis[dfwis.Model == 'FluSight-baseline'] \n",
    "baseline = baseline.rename(columns={'wis':'wis_baseline', 'Model':'baseline'})\n",
    "dfwis_test = dfwis[dfwis.Model != 'FluSight-baseline']\n",
    "\n",
    "dfwis_ratio = pd.merge(dfwis_test, baseline, how='inner',\n",
    "                       on = ['location', 'target_end_date', 'horizon', 'reference_date'])\n",
    "\n",
    "# calculate wis ratio\n",
    "dfwis_ratio['wis_ratio'] = dfwis_ratio['wis']/dfwis_ratio['wis_baseline']\n",
    "\n",
    "# save to csv\n",
    "print('Saving WIS ratio to file...')\n",
    "dfwis_ratio.to_csv('./scratch/WIS_ratio.csv', index=False, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0eaabb-7a71-404d-811a-a3868978ee06",
   "metadata": {},
   "source": [
    "### Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c76d9b-36a1-4692-a0b3-4cc2c1d1ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coverage for all forecasts\n",
    "print('Calculating coverage...')\n",
    "\n",
    "def batch_coverage(model, date, loc, horizon, verbose=False):\n",
    "    '''\n",
    "    Calculate coverage for given model, date, location, and horizon.\n",
    "    Assumes Scoring class is defined and formatted predsall dataframe is available.\n",
    "    \n",
    "    Arguments:\n",
    "      model   - model name as string\n",
    "      date    - reference date as formatted in predsall\n",
    "      loc     - location code as string\n",
    "      horizon - horizon as int\n",
    "      verbose - if True prints message if data exist and scoring is completed\n",
    "    \n",
    "    Returns:\n",
    "      formatted dataframe of scores\n",
    "    '''\n",
    "    # filter by model and submission date, only look at horizon 0-3\n",
    "    pred = predsall[(predsall.Model == model)& (predsall.reference_date == date) &\\\n",
    "                    (predsall.horizon == horizon) & (predsall.location == loc)]\n",
    "\n",
    "    if len(pred) == 0: return\n",
    "\n",
    "    test = Scoring(df=pred, obsdf=surv, target='hosp')\n",
    "    predss = test.process_predictions(pred, t_col = 'target_end_date', quantile_col = 'output_type_id')\n",
    "\n",
    "    obs = test.get_observations(loc)\n",
    "    obs = test.process_observations(obs[obs.date.isin(pred.target_end_date.unique())])\n",
    "\n",
    "    if len(obs) == 0: return\n",
    "\n",
    "    out = test.all_coverages_from_df(obs, predss)\n",
    "\n",
    "    out['horizon'] = horizon\n",
    "    out['Model'] = model\n",
    "    out['reference_date'] = date\n",
    "    out['location'] = loc\n",
    "\n",
    "    if verbose: print(f'Coverage completed {model} {date} location {loc} horizon {horizon}', flush=True)\n",
    "    \n",
    "    return pd.DataFrame(out,index=[0])\n",
    "\n",
    "dfcoverage = pd.DataFrame()\n",
    "with mp.Pool() as pool:\n",
    "    import os\n",
    "    print(f'{len(os.sched_getaffinity(0))} cores available', flush=True)\n",
    "    report_memory()\n",
    "    arguments = set(_ for _ in predsall[['Model','reference_date','location','horizon']].itertuples(index=False, name=None))\n",
    "    scores = pool.starmap(batch_coverage, arguments)\n",
    "    dfcoverage = pd.concat(scores)\n",
    "dfcoverage = dfcoverage.reset_index().drop(columns='index')\n",
    "\n",
    "# save to csv\n",
    "print('Saving coverage to file...')\n",
    "dfcoverage.to_csv('./scratch/coverage.csv', index=False, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70997d90-60f8-4085-88a6-ffe15849043c",
   "metadata": {},
   "source": [
    "### MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21816514-8328-4fa2-bdcc-963f632d9861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MAPE for all forecasts\n",
    "print('Calculating MAPE...')\n",
    "\n",
    "def batch_mape(model, date, horizon, verbose=False):\n",
    "    '''\n",
    "    Calculate MAPE for given model, date, and horizon.\n",
    "    Assumes Scoring class is defined and formatted predsall dataframe is available.\n",
    "    \n",
    "    Arguments:\n",
    "      model   - model name as string\n",
    "      date    - reference date as formatted in predsall\n",
    "      horizon - horizon as int\n",
    "      verbose - if True prints message if data exist and scoring is completed\n",
    "    \n",
    "    Returns:\n",
    "      formatted dataframe of scores\n",
    "    '''\n",
    "    # filter by horizon, model and submission date\n",
    "    pred = predsall[(predsall.horizon==horizon) & (predsall.Model == model) & \\\n",
    "                    (predsall.reference_date == date)]\n",
    "    \n",
    "    if len(pred)==0: return\n",
    "    \n",
    "    # calculate mape for each week\n",
    "    test = Scoring(df=pred, obsdf=surv, target='hosp')\n",
    "\n",
    "    out = test.get_mape()\n",
    "    \n",
    "    out['horizon'] = horizon\n",
    "    out['reference_date'] = date\n",
    "    \n",
    "    if verbose: print(f'MAPE completed {model} {date} horizon {horizon}', flush=True)\n",
    "    \n",
    "    return out\n",
    "\n",
    "dfmape = pd.DataFrame()\n",
    "with mp.Pool() as pool:\n",
    "    import os\n",
    "    print(f'{len(os.sched_getaffinity(0))} cores available', flush=True)\n",
    "    report_memory()\n",
    "    arguments = set(_ for _ in predsall[['Model','reference_date','horizon']].itertuples(index=False, name=None))\n",
    "    scores = pool.starmap(batch_mape, arguments)\n",
    "    dfmape = pd.concat(scores)         \n",
    "\n",
    "# save to csv\n",
    "print('Saving MAPE to file...')\n",
    "dfmape.to_csv('./scratch/MAPE.csv', index=False, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
